# ==============================================================
# üìä Dashboard de Resultados ‚Äî Banana Quality
# ==============================================================

import os
import json
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ==============================================================
# Configura√ß√£o da P√°gina
# ==============================================================
st.set_page_config(page_title="Dashboard de Resultados", page_icon="üìä", layout="wide")

st.title("üìä Dashboard de Resultados ‚Äî Compara√ß√£o de Modelos")
st.markdown("""
Este painel apresenta a **s√≠ntese final dos resultados** dos modelos treinados, 
permitindo identificar **qual modelo apresentou o melhor desempenho geral** 
para o problema de classifica√ß√£o da **Qualidade das Bananas üçå**.
""")

# ==============================================================
# Carregar m√©tricas de todos os modelos
# ==============================================================
metrics_dir = os.path.join("outputs", "metrics")

model_files = {
    "Regress√£o Log√≠stica": "logistic_regression_metrics.json",
    "Random Forest": "random_forest_metrics.json",
    "KNN": "knn_classifier_metrics.json",
    "SVM": "svm_classifier_metrics.json",
    "Gradient Boosting": "gradient_boosting_metrics.json",
    "XGBoost": "xgboost_metrics.json",
}

metrics = {}
for model_name, file_name in model_files.items():
    path = os.path.join(metrics_dir, file_name)
    if os.path.exists(path):
        with open(path, "r") as f:
            metrics[model_name] = json.load(f)

if not metrics:
    st.error("‚ö†Ô∏è Nenhum arquivo de m√©tricas encontrado em `outputs/metrics`.")
    st.stop()

# ==============================================================
# Consolidar m√©tricas
# ==============================================================
df_metrics = pd.DataFrame(metrics).T.round(4)
st.subheader("üìã Tabela Consolidada de M√©tricas")
st.dataframe(df_metrics, use_container_width=True)

# ==============================================================
# Gr√°ficos de Compara√ß√£o
# ==============================================================
st.subheader("üìä Compara√ß√£o Visual das M√©tricas")
metrics_to_plot = ["Acur√°cia", "F1-score", "Precis√£o", "Recall", "RMSE", "R¬≤"]

for metric in metrics_to_plot:
    if metric in df_metrics.columns:
        fig, ax = plt.subplots(figsize=(8, 4))
        sns.barplot(x=df_metrics.index, y=df_metrics[metric], palette="viridis", ax=ax)
        ax.set_title(f"Compara√ß√£o ‚Äî {metric}")
        ax.set_ylabel(metric)
        ax.set_xlabel("Modelo")
        ax.bar_label(ax.containers[0], fmt="%.3f", padding=3)
        st.pyplot(fig)

# ==============================================================
# Gr√°fico Radar (Spider Chart)
# ==============================================================
st.subheader("üï∏Ô∏è Gr√°fico Radar ‚Äî Desempenho Multidimensional dos Modelos")

# Selecionar apenas m√©tricas positivas para compara√ß√£o visual
radar_metrics = ["Acur√°cia", "Precis√£o", "Recall", "F1-score"]
radar_df = df_metrics[radar_metrics]

# Normalizar valores entre 0 e 1 para melhor visualiza√ß√£o
radar_norm = (radar_df - radar_df.min()) / (radar_df.max() - radar_df.min())

# Configurar √¢ngulos do gr√°fico
labels = list(radar_norm.columns)
num_vars = len(labels)
angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]  # fechar o c√≠rculo

# Criar gr√°fico
fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))
for model in radar_norm.index:
    values = radar_norm.loc[model].tolist()
    values += values[:1]
    ax.plot(angles, values, label=model, linewidth=2)
    ax.fill(angles, values, alpha=0.15)

ax.set_yticklabels([])
ax.set_xticks(angles[:-1])
ax.set_xticklabels(labels, fontsize=10)
ax.set_title("Radar de Desempenho dos Modelos", size=14, pad=20)
ax.legend(loc="upper right", bbox_to_anchor=(1.3, 1.1))
st.pyplot(fig)

# ==============================================================
# Identificar o melhor modelo
# ==============================================================
st.subheader("üèÜ Conclus√£o dos Resultados")

best_accuracy = df_metrics["Acur√°cia"].idxmax()
best_f1 = df_metrics["F1-score"].idxmax()
best_r2 = df_metrics["R¬≤"].idxmax()
best_rmse = df_metrics["RMSE"].idxmin()

col1, col2 = st.columns(2)
with col1:
    st.success(f"üéØ **Melhor Acur√°cia:** {best_accuracy}")
    st.info(f"üí™ **Melhor F1-score:** {best_f1}")
with col2:
    st.warning(f"üìà **Melhor R¬≤:** {best_r2}")
    st.error(f"üìâ **Menor RMSE (erro):** {best_rmse}")

# ==============================================================
# Conclus√£o Autom√°tica
# ==============================================================
st.markdown("### üìö Interpreta√ß√£o dos Resultados")

conclusao = f"""
Ap√≥s a compara√ß√£o de seis algoritmos de aprendizado supervisionado, observou-se que o modelo **{best_accuracy}**
apresentou o **melhor desempenho geral**, alcan√ßando a maior acur√°cia e F1-score. 

Modelos como **{best_r2}** tamb√©m mostraram bom poder de explica√ß√£o da variabilidade (R¬≤ elevado), enquanto **{best_rmse}**
apresentou o menor erro m√©dio quadr√°tico (RMSE), indicando boa capacidade preditiva.

Esses resultados sugerem que o modelo **{best_accuracy}** √© o mais indicado para uso pr√°tico
na **predi√ß√£o da qualidade das bananas**, equilibrando desempenho e estabilidade.
"""

st.markdown(conclusao)
